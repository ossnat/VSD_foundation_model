{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VidoeMAE 2D CNN Training Notebook\n",
        "\n",
        "This notebook trains a 2D MAE model using the latest dataset strucure:\n",
        "- Build a 2D MAE (shallow CNN encoder + lightweight decoder)\n",
        "- Train and log metrics to TensorBoard\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm -rf VSD_foundation_model\n",
        "!git clone --branch sharpen_dataset_and_dataloader https://github.com/ossnat/VSD_foundation_model.git\n",
        "!ln -s '/content/drive/My Drive/VSD_FM/Data' '/content'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and configuration\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from pathlib import Path\n",
        "\n",
        "project_root = Path('VSD_foundation_model')\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "from src.data import load_dataset, create_dataset, VsdVideoDataset\n",
        "from src.models.heads.mae_decoder_2d import MAEDecoder2D\n",
        "from src.models.systems.mae_system import MAESystem\n",
        "from src.utils.logger import TBLogger, set_seed\n",
        "from src.training.trainer import Trainer\n",
        "\n",
        "# Shallow CNN backbone with ResNet18-like output size\n",
        "class MAEShallowCNNBackbone(nn.Module):\n",
        "    def __init__(self, in_channels: int = 1):\n",
        "        super().__init__()\n",
        "        self.feature_dim = 512\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.encoder(x)\n",
        "\n",
        "# Load config\n",
        "config_path = \"VSD_foundation_model/configs/VideoMAE_2D.yaml\"\n",
        "with open(config_path, 'r') as f:\n",
        "  cfg = yaml.safe_load(f)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\nUsing device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size=32\n",
        "train_loader = load_dataset(cfg, split=\"train\", batch_size=batch_size, num_workers=0, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_loader = load_dataset(cfg, split=\"val\", batch_size=batch_size, num_workers=0, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loader = load_dataset(cfg, split=\"test\", batch_size=batch_size, num_workers=0, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from src.models.backbone.mae_backbone_3d import MAER3D18Backbone\n",
        "# from src.models.heads.mae_decoder_3d import MAEDecoder3D\n",
        "\n",
        "# Build MAE 2D model and optimizer\n",
        "encoder = MAEShallowCNNBackbone(in_channels=1)\n",
        "decoder = MAEDecoder2D(in_channels=encoder.feature_dim, out_channels=1, hidden_dim=256)\n",
        "\n",
        "# Load config\n",
        "model_config_path = \"VSD_foundation_model/configs/model_configs/mae_2d_config.yaml\"\n",
        "with open(config_path, 'r') as f:\n",
        "  model_cfg = yaml.safe_load(f)\n",
        "\n",
        "\n",
        "model = MAESystem(encoder=encoder, decoder=decoder, config=model_cfg).to(device)\n",
        "# Freeze encoder\n",
        "for p in model.encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# (Optional) keep decoder trainable\n",
        "for p in model.decoder.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# Build optimizer on decoder params only\n",
        "optimizer = torch.optim.AdamW(model.decoder.parameters(), lr=1e-4, weight_decay=0.05)\n",
        "# optimizer = model.get_optimizer() # If you wish to train also encoder\n",
        "\n",
        "print(model.__class__.__name__, \"built.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sanity check: forward pass with NaN/Inf detection per layer\n",
        "+model.eval()\n",
        "+\n",
        "+batch = next(iter(train_loader))\n",
        "+batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
        "+\n",
        "+def _check_finite(tensor, label):\n",
        "+    if torch.is_tensor(tensor):\n",
        "+        finite = torch.isfinite(tensor)\n",
        "+        if not finite.all():\n",
        "+            bad = (~finite).sum().item()\n",
        "+            print(f\"❌ Non-finite values in {label}: {bad} elements\")\n",
        "+\n",
        "+# Check initial input\n",
        "+_check_finite(batch.get(\"video_masked\"), \"input video_masked\")\n",
        "+\n",
        "+handles = []\n",
        "+\n",
        "+def _hook_fn(module, inputs, output):\n",
        "+    name = module.__class__.__name__\n",
        "+    # Check inputs\n",
        "+    if isinstance(inputs, (list, tuple)):\n",
        "+        for i, inp in enumerate(inputs):\n",
        "+            _check_finite(inp, f\"{name} input[{i}]\")\n",
        "+    else:\n",
        "+        _check_finite(inputs, f\"{name} input\")\n",
        "+    # Check outputs\n",
        "+    if isinstance(output, (list, tuple)):\n",
        "+        for i, out in enumerate(output):\n",
        "+            _check_finite(out, f\"{name} output[{i}]\")\n",
        "+    else:\n",
        "+        _check_finite(output, f\"{name} output\")\n",
        "+\n",
        "+for module in model.modules():\n",
        "+    handles.append(module.register_forward_hook(_hook_fn))\n",
        "+\n",
        "+with torch.no_grad():\n",
        "+    _ = model(batch)\n",
        "+\n",
        "+for h in handles:\n",
        "+    h.remove()\n",
        "+\n",
        "+print(\"✅ NaN/Inf check completed for initial input and all layers.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train using generic Trainer (supports MAE 2D/3D and DINO)\n",
        "logger = TBLogger(log_dir=\"logs\")\n",
        "trainer = Trainer(model=model, logger=logger, cfg=model_cfg, device=device)\n",
        "trainer.fit(train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "\n",
        "# Define the log directory (matching the one used by TBLogger)\n",
        "log_dir = \"logs\"\n",
        "\n",
        "# Find the latest event file in the log directory\n",
        "event_file = None\n",
        "for root, dirs, files in os.walk(log_dir):\n",
        "    for f in files:\n",
        "        if \"events.out.tfevents\" in f:\n",
        "            event_file = os.path.join(root, f)\n",
        "            break\n",
        "    if event_file:\n",
        "        break\n",
        "\n",
        "if event_file:\n",
        "    print(f\"Found TensorBoard event file: {event_file}\")\n",
        "    # Initialize EventAccumulator to read logs\n",
        "    event_acc = EventAccumulator(event_file)\n",
        "    event_acc.Reload()\n",
        "\n",
        "    # Get scalar data for 'train/loss'\n",
        "    if 'train/loss' in event_acc.Tags()['scalars']:\n",
        "        loss_events = event_acc.Scalars('train/loss')\n",
        "        losses = [event.value for event in loss_events]\n",
        "        steps = [event.step for event in loss_events]\n",
        "\n",
        "        if len(losses) > 0:\n",
        "            print(f\"Found {len(losses)} loss values.\")\n",
        "\n",
        "            # Calculate rolling mean for every 10 iterations\n",
        "            window_size = 10\n",
        "            if len(losses) >= window_size:\n",
        "                rolling_means = []\n",
        "                rolling_steps = []\n",
        "                for i in range(len(losses) - window_size + 1):\n",
        "                    rolling_means.append(np.mean(losses[i : i + window_size]))\n",
        "                    # Use the last step of the window for plotting\n",
        "                    rolling_steps.append(steps[i + window_size - 1])\n",
        "\n",
        "                # Plotting the rolling mean\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                plt.plot(rolling_steps, rolling_means, label=f'Train Loss (Rolling Mean over {window_size} iterations)')\n",
        "                plt.xlabel('Iteration')\n",
        "                plt.ylabel('Loss')\n",
        "                plt.title(f'Train Loss Rolling Mean (Every {window_size} Iterations)')\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                plt.show()\n",
        "                print(f\"✅ Plotted rolling mean of loss over {window_size} iterations.\")\n",
        "            else:\n",
        "                print(f\"Not enough loss data ({len(losses)} points) to calculate rolling mean over {window_size} iterations. Plotting raw loss instead.\")\n",
        "                # Plot raw loss if not enough for rolling mean\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                plt.plot(steps, losses, label='Train Loss (Raw)')\n",
        "                plt.xlabel('Iteration')\n",
        "                plt.ylabel('Loss')\n",
        "                plt.title('Train Loss (Raw values)')\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                plt.show()\n",
        "                print(\"✅ Plotted raw loss values.\")\n",
        "        else:\n",
        "            print(\"No 'train/loss' scalar data found in event file.\")\n",
        "    else:\n",
        "        print(\"No 'train/loss' scalar tag found in event file.\")\n",
        "        print(f\"Available scalar tags: {event_acc.Tags()['scalars']}\")\n",
        "else:\n",
        "    print(f\"No TensorBoard event files found in '{log_dir}'. Training might not have generated logs yet or was interrupted too early.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def check_nan_in_weights(model):\n",
        "    print(\"Checking for NaN values in model weights...\")\n",
        "    nan_found = False\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad and param.data is not None:\n",
        "            # Move to CPU and convert to numpy for NaN check\n",
        "            data = param.data.detach().cpu().numpy()\n",
        "            if np.isnan(data).any():\n",
        "                print(f\"⚠️ NaN values found in weights of: {name}\")\n",
        "                nan_found = True\n",
        "            else:\n",
        "                print(f\"No NaN values in weights of: {name}\")\n",
        "        else:\n",
        "            print(f\"Skipping {name}: Not a trainable parameter or no data.\")\n",
        "\n",
        "    if not nan_found:\n",
        "        print(\"✅ No NaN values found in any trainable model weights.\")\n",
        "    else:\n",
        "        print(\"❌ NaN values detected in some model weights.\")\n",
        "\n",
        "check_nan_in_weights(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def plot_weights_histogram(model):\n",
        "    print(\"Plotting histograms for model weights...\")\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad and param.data is not None:\n",
        "            data = param.data.detach().cpu().numpy()\n",
        "            # Flatten the data to plot a single histogram\n",
        "            data = data.reshape(-1)\n",
        "\n",
        "            # Filter out NaN values if any (though unlikely for weights)\n",
        "            data = data[~np.isnan(data)]\n",
        "\n",
        "            if data.size > 0:\n",
        "                plt.figure(figsize=(8, 5))\n",
        "                plt.hist(data, bins=100, color='skyblue', edgecolor='black')\n",
        "                plt.title(f'Histogram of Weights for: {name}')\n",
        "                plt.xlabel('Weight Value')\n",
        "                plt.ylabel('Frequency')\n",
        "                plt.grid(True)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "            else:\n",
        "                print(f\"Skipping {name}: No valid weight data to plot.\")\n",
        "        else:\n",
        "            print(f\"Skipping {name}: Not a trainable parameter or no data.\")\n",
        "    print(\"✅ All weight histograms plotted.\")\n",
        "\n",
        "plot_weights_histogram(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "# Sequence visualization: plot Original, Masked(+overlay), Reconstructed vertically over time\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a minimal version\n",
        "def get_reconstruction(model, batch):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        video_masked = batch[\"video_masked\"]\n",
        "        video_target = batch[\"video_target\"]\n",
        "        mask = batch[\"mask\"]\n",
        "        is_2d = False\n",
        "        if len(video_target.shape) == 5 and video_target.shape[2] == 1:\n",
        "            is_2d = True\n",
        "            video_masked = video_masked.squeeze(2)\n",
        "            video_target = video_target.squeeze(2)\n",
        "            if len(mask.shape) == 5:\n",
        "                mask = mask.squeeze(2)\n",
        "            if len(mask.shape) == 4 and mask.shape[1] == 1:\n",
        "                H, W = video_target.shape[2], video_target.shape[3]\n",
        "                mask = F.interpolate(mask, size=(H, W), mode='nearest')\n",
        "        features = model.encoder(video_masked)\n",
        "        if is_2d or len(video_target.shape) == 4:\n",
        "            target_size = (video_target.shape[2], video_target.shape[3])\n",
        "            reconstruction = model.decoder(features, target_size=target_size)\n",
        "        else:\n",
        "            target_size = (video_target.shape[2], video_target.shape[3], video_target.shape[4])\n",
        "            reconstruction = model.decoder(features, target_size=target_size)\n",
        "            if len(mask.shape) == 5 and mask.shape[1] == 1:\n",
        "                T, H, W = video_target.shape[2], video_target.shape[3], video_target.shape[4]\n",
        "                mask = F.interpolate(mask, size=(T, H, W), mode='nearest')\n",
        "        return reconstruction, mask, video_target, video_masked\n",
        "\n",
        "# Fetch a validation batch and reconstruct\n",
        "model.eval()\n",
        "val_sample = next(iter(val_loader))\n",
        "val_sample = {k: v.to(device) if torch.is_tensor(v) else v\n",
        "                             for k, v in val_sample.items()}\n",
        "reconstruction, mask, original, masked = get_reconstruction(model, val_sample)\n",
        "\n",
        "# Determine dimensionality\n",
        "# 2D: (B,C,H,W); 3D: (B,C,T,H,W)\n",
        "\n",
        "if original.ndim == 4:  # 2D case: (B,C,H,W) - plot multiple batch items\n",
        "    print(\"Dimensionality is 2D (single frame). Plotting multiple batch items.\")\n",
        "    max_batch_items_to_plot = 10\n",
        "    num_items = min(original.shape[0], max_batch_items_to_plot)\n",
        "\n",
        "    # Calculate vmin and vmax using percentiles across all selected batch items for better visualization\n",
        "    selected_originals_flat = original[:num_items, 0, :, :].cpu().numpy().flatten()\n",
        "    vmin, vmax = np.percentile(selected_originals_flat, 10), np.percentile(selected_originals_flat, 90)\n",
        "\n",
        "    fig, axes = plt.subplots(num_items, 3, figsize=(12, 2.2 * num_items))\n",
        "    if num_items == 1:\n",
        "        axes = np.array([axes]) # Ensure axes is 2D even for single row\n",
        "\n",
        "    for b_idx in range(num_items):\n",
        "        monkey = val_sample['monkey'][b_idx]\n",
        "        date = val_sample['date'][b_idx]\n",
        "        condition = val_sample['condition'][b_idx]\n",
        "\n",
        "        # Original\n",
        "        axes[b_idx, 0].imshow(original[b_idx, 0].cpu().numpy(), cmap='hot', vmin=vmin, vmax=vmax)\n",
        "        axes[b_idx, 0].set_ylabel(f\"Sample {b_idx} {monkey} {date} {condition}\")\n",
        "        axes[b_idx, 0].set_xticks([]); axes[b_idx, 0].set_yticks([])\n",
        "        if b_idx == 0:\n",
        "            axes[b_idx, 0].set_title('Original')\n",
        "\n",
        "        # Masked + overlay\n",
        "        masked_frame = masked[b_idx, 0].cpu().numpy()\n",
        "        mask_frame = mask[b_idx, 0].cpu().numpy()\n",
        "        # Resize mask if it's smaller than the video frame (happens with pooling)\n",
        "        if mask_frame.shape != masked_frame.shape:\n",
        "             mask_frame_tensor = torch.from_numpy(mask_frame).unsqueeze(0).unsqueeze(0) # (1,1,H_mask,W_mask)\n",
        "             mask_frame = F.interpolate(mask_frame_tensor, size=masked_frame.shape, mode='nearest').squeeze().numpy()\n",
        "        overlay = np.zeros((*mask_frame.shape, 4), dtype=np.float32) # Use new mask_frame shape\n",
        "        overlay[..., 0] = 1.0\n",
        "        overlay[..., 3] = 0.3 * (1.0 - mask_frame)  # red where masked\n",
        "        axes[b_idx, 1].imshow(masked_frame, cmap='hot', vmin=vmin, vmax=vmax)\n",
        "        axes[b_idx, 1].imshow(overlay)\n",
        "        axes[b_idx, 1].set_xticks([]); axes[b_idx, 1].set_yticks([])\n",
        "        if b_idx == 0:\n",
        "            axes[b_idx, 1].set_title('Masked (overlay)')\n",
        "\n",
        "        # Reconstructed\n",
        "        recon_frame = reconstruction[b_idx, 0].cpu().numpy()\n",
        "        axes[b_idx, 2].imshow(recon_frame, cmap='hot', vmin=vmin, vmax=vmax)\n",
        "        axes[b_idx, 2].set_xticks([]); axes[b_idx, 2].set_yticks([])\n",
        "        if b_idx == 0:\n",
        "            axes[b_idx, 2].set_title('Reconstructed')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"✅ Sequence visualization complete (rows=batch_items, cols=[orig, masked+overlay, recon]).\")\n",
        "\n",
        "else:  # 3D case: (B,C,T,H,W) - plot first batch item over time\n",
        "    print(\"Dimensionality is 3D (video). Plotting first batch item over time.\")\n",
        "    b = 0 # Select first item in batch\n",
        "\n",
        "    # Calculate vmin and vmax using percentiles for better visualization\n",
        "    original_flat = original[b, 0, :, :, :].cpu().numpy().flatten() # For 3D, original has B,C,T,H,W\n",
        "    vmin, vmax = np.percentile(original_flat, 10), np.percentile(original_flat, 90)\n",
        "\n",
        "    T = original.shape[2]\n",
        "    orig_frames = original[b, 0].cpu()          # (T,H,W)\n",
        "    masked_frames = masked[b, 0].cpu()          # (T,H,W)\n",
        "    recon_frames = reconstruction[b, 0].cpu()   # (T,H,W)\n",
        "    mask_frames = mask[b, 0].cpu()              # (T,H,W) after model fix (assuming get_reconstruction handles resize)\n",
        "\n",
        "    # Limit number of frames for display\n",
        "    max_rows = 12\n",
        "    rows = min(T, max_rows)\n",
        "    fig, axes = plt.subplots(rows, 3, figsize=(12, 2.2*rows))\n",
        "    if rows == 1:\n",
        "        axes = np.array([axes]) # Ensure axes is 2D even for single row\n",
        "\n",
        "    for t in range(rows):\n",
        "        # Original\n",
        "        axes[t, 0].imshow(orig_frames[t].cpu().numpy() if torch.is_tensor(orig_frames[t]) else orig_frames[t], cmap='hot', vmin=vmin, vmax=vmax)\n",
        "        axes[t, 0].set_ylabel(f\"t={t}\")\n",
        "        axes[t, 0].set_xticks([]); axes[t, 0].set_yticks([])\n",
        "        if t == 0:\n",
        "            axes[t, 0].set_title('Original')\n",
        "\n",
        "        # Masked + overlay\n",
        "        im = axes[t, 1].imshow(masked_frames[t].cpu().numpy() if torch.is_tensor(masked_frames[t]) else masked_frames[t], cmap='hot', vmin=vmin, vmax=vmax)\n",
        "        m = mask_frames[t].cpu().numpy() if torch.is_tensor(mask_frames[t]) else mask_frames[t]\n",
        "        # Check if mask_frame needs interpolation (e.g., if mask is patch-based and smaller than image)\n",
        "        if m.shape != masked_frames[t].shape:\n",
        "            m_tensor = torch.from_numpy(m).unsqueeze(0).unsqueeze(0) # (1,1,H_mask,W_mask)\n",
        "            m = F.interpolate(m_tensor, size=masked_frames[t].shape, mode='nearest').squeeze().numpy()\n",
        "        overlay = np.zeros((m.shape[0], m.shape[1], 4), dtype=np.float32) # Use new m shape\n",
        "        overlay[..., 0] = 1.0\n",
        "        overlay[..., 3] = 0.3 * (1.0 - m)  # red where masked\n",
        "        axes[t, 1].imshow(overlay)\n",
        "        axes[t, 1].set_xticks([]); axes[t, 1].set_yticks([])\n",
        "        if t == 0:\n",
        "            axes[t, 1].set_title('Masked (overlay)')\n",
        "\n",
        "        # Reconstructed\n",
        "        axes[t, 2].imshow(recon_frames[t].cpu().numpy() if torch.is_tensor(recon_frames[t]) else recon_frames[t], cmap='hot', vmin=vmin, vmax=vmax)\n",
        "        axes[t, 2].set_xticks([]); axes[t, 2].set_yticks([])\n",
        "        if t == 0:\n",
        "            axes[t, 2].set_title('Reconstructed')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"✅ Sequence visualization complete (rows=time, cols=[orig, masked+overlay, recon]).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "def plot_recon_hist(recon_frames, bins=100, title=\"Reconstruction pixel histogram\"):\n",
        "    # Move tensor to CPU and flatten\n",
        "    if torch.is_tensor(recon_frames):\n",
        "        data = recon_frames.detach().cpu().numpy()\n",
        "    else:\n",
        "        data = np.asarray(recon_frames)\n",
        "\n",
        "    data = data.reshape(-1)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.hist(data, bins=bins, color=\"steelblue\", alpha=0.8)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Pixel value\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_recon_hist(recon_frames)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

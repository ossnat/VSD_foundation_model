{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh4_FAxj6luD"
      },
      "source": [
        "# VideoMAE 2D Training Notebook\n",
        "\n",
        "This notebook trains a 2D MAE model using an HDF5 dataset:\n",
        "- Load an HDF5 file from a path\n",
        "- Split trials into train/val using `split_data`\n",
        "- Create datasets via `create_dataset('vsd_mae', ...)` and DataLoaders\n",
        "- Build a 2D MAE (ResNet18 backbone + lightweight decoder)\n",
        "- Train and log metrics to TensorBoard\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLcJ6_LX6ySK",
        "outputId": "dd9b4728-6e87-48c6-9ff7-51ed4c905f18"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf VSD_foundation_model\n",
        "!git clone --branch None_to_null https://github.com/ossnat/VSD_foundation_model.git\n",
        "!ln -s '/content/drive/My Drive/VSD_FM/Data' '/content'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfImauQI60A_",
        "outputId": "3bdd6296-5ebf-4c32-c6a9-7ce87fcedfed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'VSD_foundation_model'...\n",
            "remote: Enumerating objects: 589, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 589 (delta 7), reused 0 (delta 0), pack-reused 544 (from 1)\u001b[K\n",
            "Receiving objects: 100% (589/589), 5.54 MiB | 9.37 MiB/s, done.\n",
            "Resolving deltas: 100% (348/348), done.\n",
            "ln: failed to create symbolic link '/content/Data': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoSKXP1X6luE",
        "outputId": "4c67bc99-20dc-4118-d798-a68cb0fce8a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Imports and configuration\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from pathlib import Path\n",
        "\n",
        "project_root = Path('VSD_foundation_model')\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "from src.data import load_dataset, create_dataset, VsdVideoDataset\n",
        "from src.models.backbone.mae_backbone_2d import MAEResNet18Backbone\n",
        "from src.models.heads.mae_decoder_2d import MAEDecoder2D\n",
        "from src.models.systems.mae_system import MAESystem\n",
        "from src.utils.logger import TBLogger, set_seed\n",
        "from src.training.trainer import Trainer\n",
        "\n",
        "# Load config\n",
        "config_path = \"VSD_foundation_model/configs/VideoMAE_2D.yaml\"\n",
        "with open(config_path, 'r') as f:\n",
        "  cfg = yaml.safe_load(f)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\nUsing device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32\n",
        "train_loader = load_dataset(cfg, split=\"train\", batch_size=batch_size, num_workers=0, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENOaQLdSBsK6",
        "outputId": "bf904d45-4424-49e5-8a7b-0079da364495"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading split CSV from Data/FoundationData/ProcessedData/splits/split_v1_seed17_strat_monkey.csv...\n",
            "Successfully read CSV with standard settings\n",
            "Found CSV columns (10 total): ['trial_global_id', 'monkey', 'date', 'condition', 'source_file', 'target_file', 'trial_index_in_condition', 'shape', 'trial_dataset', 'split']\n",
            "CSV shape: (5706, 10) (rows x columns)\n",
            "Required columns found: ['target_file', 'trial_dataset', 'shape', 'split'] out of ['target_file', 'trial_dataset', 'shape', 'split']\n",
            "Sample first row (showing required columns):\n",
            "  target_file: Data/FoundationData/ProcessedData/boromir/session_220622d_condsAN.h5\n",
            "  trial_dataset: trial_000008\n",
            "  shape: (10000, 256)\n",
            "  split: train\n",
            "Found 3994 trials for split 'train'\n",
            "Loading stats from Data/FoundationData/ProcessedData/splits/baseline_stats_v1_seed17_strat_monkey.json...\n",
            "Loading mean and std from Data/FoundationData/ProcessedData/splits/baseline_stats_v1_seed17_strat_monkey.h5...\n",
            "Loaded normalization stats from H5 file:\n",
            "  Mean range: [0.9996, 1.0004], shape: torch.Size([1, 1, 100, 100])\n",
            "  Std range: [0.0002, 0.0061], shape: torch.Size([1, 1, 100, 100])\n",
            "Created 483274 samples from 3994 trials\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = load_dataset(cfg, split=\"val\", batch_size=batch_size, num_workers=0, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lokpLS7IE_Ee",
        "outputId": "afe062df-8b13-41ec-a2fa-1cf3f7506a85"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading split CSV from Data/FoundationData/ProcessedData/splits/split_v1_seed17_strat_monkey.csv...\n",
            "Successfully read CSV with standard settings\n",
            "Found CSV columns (10 total): ['trial_global_id', 'monkey', 'date', 'condition', 'source_file', 'target_file', 'trial_index_in_condition', 'shape', 'trial_dataset', 'split']\n",
            "CSV shape: (5706, 10) (rows x columns)\n",
            "Required columns found: ['target_file', 'trial_dataset', 'shape', 'split'] out of ['target_file', 'trial_dataset', 'shape', 'split']\n",
            "Sample first row (showing required columns):\n",
            "  target_file: Data/FoundationData/ProcessedData/boromir/session_220622d_condsAN.h5\n",
            "  trial_dataset: trial_000008\n",
            "  shape: (10000, 256)\n",
            "  split: train\n",
            "Found 856 trials for split 'val'\n",
            "Loading stats from Data/FoundationData/ProcessedData/splits/baseline_stats_v1_seed17_strat_monkey.json...\n",
            "Loading mean and std from Data/FoundationData/ProcessedData/splits/baseline_stats_v1_seed17_strat_monkey.h5...\n",
            "Loaded normalization stats from H5 file:\n",
            "  Mean range: [0.9996, 1.0004], shape: torch.Size([1, 1, 100, 100])\n",
            "  Std range: [0.0002, 0.0061], shape: torch.Size([1, 1, 100, 100])\n",
            "Created 103576 samples from 856 trials\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = load_dataset(cfg, split=\"test\", batch_size=batch_size, num_workers=0, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWspLIUOFBHZ",
        "outputId": "abbdb8f2-b5ba-4d01-bd70-43da233ac68e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading split CSV from Data/FoundationData/ProcessedData/splits/split_v1_seed17_strat_monkey.csv...\n",
            "Successfully read CSV with standard settings\n",
            "Found CSV columns (10 total): ['trial_global_id', 'monkey', 'date', 'condition', 'source_file', 'target_file', 'trial_index_in_condition', 'shape', 'trial_dataset', 'split']\n",
            "CSV shape: (5706, 10) (rows x columns)\n",
            "Required columns found: ['target_file', 'trial_dataset', 'shape', 'split'] out of ['target_file', 'trial_dataset', 'shape', 'split']\n",
            "Sample first row (showing required columns):\n",
            "  target_file: Data/FoundationData/ProcessedData/boromir/session_220622d_condsAN.h5\n",
            "  trial_dataset: trial_000008\n",
            "  shape: (10000, 256)\n",
            "  split: train\n",
            "Found 856 trials for split 'test'\n",
            "Loading stats from Data/FoundationData/ProcessedData/splits/baseline_stats_v1_seed17_strat_monkey.json...\n",
            "Loading mean and std from Data/FoundationData/ProcessedData/splits/baseline_stats_v1_seed17_strat_monkey.h5...\n",
            "Loaded normalization stats from H5 file:\n",
            "  Mean range: [0.9996, 1.0004], shape: torch.Size([1, 1, 100, 100])\n",
            "  Std range: [0.0002, 0.0061], shape: torch.Size([1, 1, 100, 100])\n",
            "Created 103576 samples from 856 trials\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qjgjf7m46luG",
        "outputId": "efdd0127-6ba2-4de4-88de-e4d450f082d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAESystem built.\n"
          ]
        }
      ],
      "source": [
        "# from src.models.backbone.mae_backbone_3d import MAER3D18Backbone\n",
        "# from src.models.heads.mae_decoder_3d import MAEDecoder3D\n",
        "\n",
        "# Build MAE 2D model and optimizer\n",
        "encoder = MAEResNet18Backbone(pretrained=False, in_channels=1)\n",
        "decoder = MAEDecoder2D(in_channels=encoder.feature_dim, out_channels=1, hidden_dim=256)\n",
        "\n",
        "# Load config\n",
        "model_config_path = \"VSD_foundation_model/configs/model_configs/mae_2d_config.yaml\"\n",
        "with open(config_path, 'r') as f:\n",
        "  model_cfg = yaml.safe_load(f)\n",
        "\n",
        "\n",
        "model = MAESystem(encoder=encoder, decoder=decoder, config=model_cfg).to(device)\n",
        "optimizer = model.get_optimizer()\n",
        "\n",
        "print(model.__class__.__name__, \"built.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD-hLFb_6luG",
        "outputId": "a739628e-f4fa-4f42-d11c-d38b62c36af0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/VSD_foundation_model/src/training/trainer.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n",
            "Epoch 1/3:   0%|          | 0/15103 [00:00<?, ?it/s]/content/VSD_foundation_model/src/training/trainer.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Epoch 1/3:   0%|          | 44/15103 [04:33<18:54:20,  4.52s/it, loss=25.2]"
          ]
        }
      ],
      "source": [
        "# Train using generic Trainer (supports MAE 2D/3D and DINO)\n",
        "logger = TBLogger(log_dir=\"logs\")\n",
        "trainer = Trainer(model=model, logger=logger, cfg=model_cfg, device=device)\n",
        "trainer.fit(train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlGs8l396luG"
      },
      "outputs": [],
      "source": [
        "# Sequence visualization: plot Original, Masked(+overlay), Reconstructed vertically over time\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a minimal version\n",
        "def get_reconstruction(model, batch):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        video_masked = batch[\"video_masked\"]\n",
        "        video_target = batch[\"video_target\"]\n",
        "        mask = batch[\"mask\"]\n",
        "        is_2d = False\n",
        "        if len(video_target.shape) == 5 and video_target.shape[2] == 1:\n",
        "            is_2d = True\n",
        "            video_masked = video_masked.squeeze(2)\n",
        "            video_target = video_target.squeeze(2)\n",
        "            if len(mask.shape) == 5:\n",
        "                mask = mask.squeeze(2)\n",
        "            if len(mask.shape) == 4 and mask.shape[1] == 1:\n",
        "                H, W = video_target.shape[2], video_target.shape[3]\n",
        "                mask = F.interpolate(mask, size=(H, W), mode='nearest')\n",
        "        features = model.encoder(video_masked)\n",
        "        if is_2d or len(video_target.shape) == 4:\n",
        "            target_size = (video_target.shape[2], video_target.shape[3])\n",
        "            reconstruction = model.decoder(features, target_size=target_size)\n",
        "        else:\n",
        "            target_size = (video_target.shape[2], video_target.shape[3], video_target.shape[4])\n",
        "            reconstruction = model.decoder(features, target_size=target_size)\n",
        "            if len(mask.shape) == 5 and mask.shape[1] == 1:\n",
        "                T, H, W = video_target.shape[2], video_target.shape[3], video_target.shape[4]\n",
        "                mask = F.interpolate(mask, size=(T, H, W), mode='nearest')\n",
        "        return reconstruction, mask, video_target, video_masked\n",
        "\n",
        "# Fetch a validation batch and reconstruct\n",
        "model.eval()\n",
        "val_sample = next(iter(val_loader))\n",
        "val_sample = {k: v.to(DEVICE) for k, v in val_sample.items()}\n",
        "reconstruction, mask, original, masked = get_reconstruction(model, val_sample)\n",
        "\n",
        "# Select first item in batch\n",
        "b = 0\n",
        "vmin, vmax = -0.003, 0.003\n",
        "\n",
        "# Determine dimensionality\n",
        "# 2D: (C,H,W); 3D: (C,T,H,W)\n",
        "if original.ndim == 4:  # (B,C,H,W)\n",
        "    # Single frame (2D)\n",
        "    T = 1\n",
        "    orig_frames = original[b, 0][None]      # (1,H,W)\n",
        "    masked_frames = masked[b, 0][None]\n",
        "    mask_frames = mask[b, 0]\n",
        "    if mask_frames.ndim == 2:\n",
        "        mask_frames = mask_frames[None]     # (1,H,W)\n",
        "    recon_frames = reconstruction[b, 0][None]\n",
        "else:\n",
        "    # Video (3D time): (B,C,T,H,W)\n",
        "    T = original.shape[2]\n",
        "    orig_frames = original[b, 0].cpu()          # (T,H,W)\n",
        "    masked_frames = masked[b, 0].cpu()          # (T,H,W)\n",
        "    recon_frames = reconstruction[b, 0].cpu()   # (T,H,W)\n",
        "    mask_frames = mask[b, 0].cpu()              # (T,H,W) after model fix\n",
        "\n",
        "# Limit number of frames for display\n",
        "max_rows = 12\n",
        "rows = min(T, max_rows)\n",
        "fig, axes = plt.subplots(rows, 3, figsize=(12, 2.2*rows))\n",
        "if rows == 1:\n",
        "    axes = np.array([axes])\n",
        "\n",
        "for t in range(rows):\n",
        "    # Original\n",
        "    axes[t, 0].imshow(orig_frames[t].cpu().numpy() if torch.is_tensor(orig_frames[t]) else orig_frames[t], cmap='hot', vmin=vmin, vmax=vmax)\n",
        "    axes[t, 0].set_ylabel(f\"t={t}\")\n",
        "    axes[t, 0].set_xticks([]); axes[t, 0].set_yticks([])\n",
        "    if t == 0:\n",
        "        axes[t, 0].set_title('Original')\n",
        "\n",
        "    # Masked + overlay\n",
        "    im = axes[t, 1].imshow(masked_frames[t].cpu().numpy() if torch.is_tensor(masked_frames[t]) else masked_frames[t], cmap='hot', vmin=vmin, vmax=vmax)\n",
        "    m = mask_frames[t].cpu().numpy() if torch.is_tensor(mask_frames[t]) else mask_frames[t]\n",
        "    overlay = np.zeros((m.shape[0], m.shape[1], 4), dtype=np.float32)\n",
        "    overlay[..., 0] = 1.0\n",
        "    overlay[..., 3] = 0.3 * (1.0 - m)  # red where masked\n",
        "    axes[t, 1].imshow(overlay)\n",
        "    axes[t, 1].set_xticks([]); axes[t, 1].set_yticks([])\n",
        "    if t == 0:\n",
        "        axes[t, 1].set_title('Masked (overlay)')\n",
        "\n",
        "    # Reconstructed\n",
        "    axes[t, 2].imshow(recon_frames[t].cpu().numpy() if torch.is_tensor(recon_frames[t]) else recon_frames[t], cmap='hot', vmin=vmin, vmax=vmax)\n",
        "    axes[t, 2].set_xticks([]); axes[t, 2].set_yticks([])\n",
        "    if t == 0:\n",
        "        axes[t, 2].set_title('Reconstructed')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"âœ… Sequence visualization complete (rows=time, cols=[orig, masked+overlay, recon]).\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VideoMAE 2D Training Notebook\n",
        "\n",
        "This notebook trains a 2D MAE model using an HDF5 dataset:\n",
        "- Load an HDF5 file from a path\n",
        "- Split trials into train/val using `split_data`\n",
        "- Create datasets via `create_dataset('vsd_mae', ...)` and DataLoaders\n",
        "- Build a 2D MAE (ResNet18 backbone + lightweight decoder)\n",
        "- Train and log metrics to TensorBoard\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Imports and configuration\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from src.data.dataset_factory import create_dataset\n",
        "from src.data.split_data import split_data\n",
        "from src.models.backbone.mae_backbone_2d import MAEResNet18Backbone\n",
        "from src.models.heads.mae_decoder_2d import MAEDecoder2D\n",
        "from src.models.systems.mae_system import MAESystem\n",
        "from src.utils.logger import TBLogger, set_seed\n",
        "\n",
        "# --- User config ---\n",
        "HDF5_PATH = r\"G:\\My Drive\\HDF5_DATA_AFTER_PREPROCESSING2\\vsd_video_data.hdf5\"\n",
        "LOG_DIR = \"logs\"\n",
        "CKPT_DIR = \"checkpoints\"\n",
        "SEED = 42\n",
        "\n",
        "# Data params\n",
        "CLIP_LENGTH = 1           # 1 -> 2D (single frame); >1 creates clips but MAE2D squeezes T=1\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "SPLIT_RATIO = 0.8\n",
        "\n",
        "# Masking params for dataset\n",
        "MASK_RATIO = 0.75\n",
        "PATCH_SIZE = (1, 16, 16)  # (T, H, W); T=1 for 2D\n",
        "\n",
        "# Training params\n",
        "EPOCHS = 5\n",
        "LR = 1e-4\n",
        "WEIGHT_DECAY = 0.05\n",
        "\n",
        "# Setup\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "set_seed(SEED)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Global data split summary:\n",
            "  Total trials: 212\n",
            "  Train trials: 169 (79.7%)\n",
            "  Val trials: 43 (20.3%)\n",
            "Train trials: 169 | Val trials: 43\n"
          ]
        }
      ],
      "source": [
        "# Split data into train/val by trials\n",
        "train_trials, val_trials, index_entries = split_data(HDF5_PATH, split_ratio=SPLIT_RATIO, random_seed=SEED)\n",
        "print(f\"Train trials: {len(train_trials)} | Val trials: {len(val_trials)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 43095 | Val samples: 10965\n"
          ]
        }
      ],
      "source": [
        "# Build datasets and dataloaders\n",
        "train_ds = create_dataset(\n",
        "    \"vsd_mae\",\n",
        "    hdf5_path=HDF5_PATH,\n",
        "    clip_length=CLIP_LENGTH,\n",
        "    trial_indices=train_trials,\n",
        "    index_entries=index_entries,\n",
        "    normalize=False,\n",
        "    mask_ratio=MASK_RATIO,\n",
        "    patch_size=PATCH_SIZE,\n",
        ")\n",
        "\n",
        "val_ds = create_dataset(\n",
        "    \"vsd_mae\",\n",
        "    hdf5_path=HDF5_PATH,\n",
        "    clip_length=CLIP_LENGTH,\n",
        "    trial_indices=val_trials,\n",
        "    index_entries=index_entries,\n",
        "    normalize=False,\n",
        "    mask_ratio=MASK_RATIO,\n",
        "    patch_size=PATCH_SIZE,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_ds)} | Val samples: {len(val_ds)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAESystem built.\n"
          ]
        }
      ],
      "source": [
        "# Build MAE 2D model and optimizer\n",
        "encoder = MAEResNet18Backbone(pretrained=False, in_channels=1)\n",
        "decoder = MAEDecoder2D(in_channels=encoder.feature_dim, out_channels=1, hidden_dim=256)\n",
        "\n",
        "config = {\n",
        "    \"training\": {\"lr\": LR, \"weight_decay\": WEIGHT_DECAY},\n",
        "    \"loss\": {\"normalize\": True},\n",
        "}\n",
        "\n",
        "model = MAESystem(encoder=encoder, decoder=decoder, config=config).to(DEVICE)\n",
        "optimizer = model.get_optimizer()\n",
        "logger = TBLogger(log_dir=LOG_DIR)\n",
        "\n",
        "print(model.__class__.__name__, \"built.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\itiel\\AppData\\Local\\Temp\\ipykernel_20820\\2637288787.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "c:\\Users\\itiel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "Epoch 1/5:   0%|          | 0/5387 [00:00<?, ?it/s]c:\\Users\\itiel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "C:\\Users\\itiel\\AppData\\Local\\Temp\\ipykernel_20820\\2637288787.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
            "Epoch 1/5:   1%|          | 44/5387 [04:04<7:24:11,  4.99s/it, loss=nan] "
          ]
        }
      ],
      "source": [
        "# Training loop with TensorBoard logging and simple validation\n",
        "from tqdm import tqdm\n",
        "\n",
        "global_step = 0\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    for batch in pbar:\n",
        "        # Move batch tensors to device and match MAESystem input keys\n",
        "        batch = {\n",
        "            \"video_masked\": batch.get(\"video_masked\", torch.zeros(1)),\n",
        "            \"video_target\": batch.get(\"video_target\", torch.zeros(1)),\n",
        "            \"mask\": batch.get(\"mask\", torch.zeros(1)),\n",
        "        }\n",
        "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            out = model(batch)\n",
        "            loss = out[\"loss\"]\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Logs\n",
        "        pbar.set_postfix({\"loss\": float(loss.item())})\n",
        "        logger.log_scalar(\"train/loss\", float(loss.item()), global_step)\n",
        "        for mname, mval in out.get(\"metrics\", {}).items():\n",
        "            logger.log_scalar(f\"train/{mname}\", mval, global_step)\n",
        "        global_step += 1\n",
        "\n",
        "    # Simple validation (average metrics over one pass)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_losses = []\n",
        "        val_metrics = {}\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            batch = {\n",
        "                \"video_masked\": batch.get(\"video_masked\", torch.zeros(1)),\n",
        "                \"video_target\": batch.get(\"video_target\", torch.zeros(1)),\n",
        "                \"mask\": batch.get(\"mask\", torch.zeros(1)),\n",
        "            }\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "            out = model(batch)\n",
        "            val_losses.append(out[\"loss\"].item())\n",
        "            for mname, mval in out.get(\"metrics\", {}).items():\n",
        "                val_metrics.setdefault(mname, []).append(mval)\n",
        "        if val_losses:\n",
        "            logger.log_scalar(\"val/loss\", sum(val_losses)/len(val_losses), epoch)\n",
        "            for mname, arr in val_metrics.items():\n",
        "                logger.log_scalar(f\"val/{mname}\", sum(arr)/len(arr), epoch)\n",
        "\n",
        "    # Save checkpoint each epoch\n",
        "    ckpt_path = os.path.join(CKPT_DIR, f\"mae2d_epoch_{epoch+1}.pt\")\n",
        "    torch.save({\n",
        "        \"encoder\": model.encoder.state_dict(),\n",
        "        \"decoder\": model.decoder.state_dict(),\n",
        "        \"config\": config,\n",
        "        \"epoch\": epoch+1,\n",
        "    }, ckpt_path)\n",
        "    print(f\"Saved checkpoint: {ckpt_path}\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

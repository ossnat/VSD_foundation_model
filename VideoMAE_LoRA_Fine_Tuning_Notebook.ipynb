{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ossnat/VSD_foundation_model/blob/main/VideoMAE_LoRA_Fine_Tuning_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A Colab notebook for fine-tuning VideoMAE on a toy video dataset using LoRA.\n",
        "# This script demonstrates the full pipeline from data loading to model training and evaluation.\n",
        "\n",
        "# -----------------\n",
        "# 1. Setup and Installs\n",
        "# -----------------\n",
        "\n",
        "# Install the required libraries. This will take a few minutes.\n",
        "!pip install -q transformers datasets accelerate peft bitsandbytes\n",
        "!pip install -q decord\n",
        "!pip install -q torch torchvision torchaudio\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\n",
        "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "from accelerate import Accelerator\n",
        "from tqdm.notebook import tqdm\n",
        "import decord\n",
        "import os\n",
        "import io\n",
        "\n",
        "# Set up the Accelerator for distributed training (useful for larger models/datasets)\n",
        "accelerator = Accelerator()\n",
        "device = accelerator.device\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"Setup complete.\")\n",
        "\n",
        "# -----------------\n",
        "# 2. Data Preparation\n",
        "# -----------------\n",
        "\n",
        "# Load a small toy video dataset from the Hugging Face Hub.\n",
        "# We'll use a subset of the UCF101 dataset to keep the runtime short.\n",
        "# It's important to use a small sample for this tutorial.\n",
        "print(\"Loading toy video dataset...\")\n",
        "dataset = load_dataset(\"hf-internal-testing/mrl-test-videos-small\", split=\"train\")\n",
        "print(\"Dataset loaded successfully.\")\n",
        "print(f\"Number of videos in the dataset: {len(dataset)}\")\n",
        "\n",
        "# Load the VideoMAE feature extractor and a pre-trained model for pre-training (masked autoencoding)\n",
        "model_name = \"MCG-NJU/videomae-base\"\n",
        "processor = VideoMAEImageProcessor.from_pretrained(model_name)\n",
        "model = VideoMAEForPreTraining.from_pretrained(model_name)\n",
        "\n",
        "# Define the number of frames to subsample and the video path key\n",
        "num_frames = 16\n",
        "video_path_key = \"video_file\"\n",
        "\n",
        "# Custom dataset class to handle video loading and processing\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, dataset, processor, num_frames, video_path_key):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "        self.num_frames = num_frames\n",
        "        self.video_path_key = video_path_key\n",
        "        # Initialize a video reader for each video to optimize loading\n",
        "        self.video_readers = {\n",
        "            os.path.basename(video_path): decord.VideoReader(io.BytesIO(video_data))\n",
        "            for video_path, video_data in zip(dataset[video_path_key], dataset[\"video_data\"])\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get video data and its reader\n",
        "        video_data = self.dataset[idx]\n",
        "        video_reader = self.video_readers[os.path.basename(video_data[self.video_path_key])]\n",
        "\n",
        "        # Determine the video length and subsample frames\n",
        "        total_frames = len(video_reader)\n",
        "        # Select num_frames evenly spaced frames\n",
        "        frame_indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
        "\n",
        "        # Read the frames\n",
        "        frames = video_reader.get_batch(frame_indices).asnumpy()\n",
        "        frames = frames.astype(np.float32)\n",
        "\n",
        "        # Preprocess the frames using the VideoMAE processor\n",
        "        pixel_values = self.processor(list(frames), return_tensors=\"pt\").pixel_values.squeeze()\n",
        "\n",
        "        # The MAE task requires masked image patches. We'll implement a simple collator for this.\n",
        "        # Here we just return the raw pixel values. Masking will happen in the data collator.\n",
        "        return {\"pixel_values\": pixel_values}\n",
        "\n",
        "# Data collator for the MAE pre-training task\n",
        "# This collator takes the frames and creates the masked and unmasked patches\n",
        "def data_collator(examples):\n",
        "    # Stack all frames from the batch\n",
        "    pixel_values = torch.stack([e[\"pixel_values\"] for e in examples])\n",
        "\n",
        "    # The VideoMAE model will handle the masking internally during the forward pass.\n",
        "    # We just need to ensure the pixel values are in the correct format and shape.\n",
        "    return {\"pixel_values\": pixel_values}\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "train_dataset = VideoDataset(dataset, processor, num_frames, video_path_key)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, collate_fn=data_collator)\n",
        "\n",
        "# -----------------\n",
        "# 3. LoRA Configuration\n",
        "# -----------------\n",
        "\n",
        "# Set up PEFT (Parameter-Efficient Fine-Tuning) with LoRA.\n",
        "# This will drastically reduce the number of trainable parameters.\n",
        "# We'll apply it to the query and value projections in the attention layers.\n",
        "peft_config = LoraConfig(\n",
        "    r=16, # Rank of the update matrices. A lower rank means fewer trainable parameters.\n",
        "    lora_alpha=32, # LoRA scaling factor.\n",
        "    target_modules=[\"query\", \"value\"], # Modules to apply LoRA to.\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "# Prepare the model for PEFT and k-bit training (optional but good practice for memory)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Print the number of trainable parameters to show the efficiency of LoRA\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# -----------------\n",
        "# 4. Training Loop\n",
        "# -----------------\n",
        "\n",
        "# We'll use a simple manual training loop here to demonstrate the process clearly.\n",
        "# For a full-scale project, you would use Hugging Face's Trainer API for more features.\n",
        "\n",
        "# Define training hyperparameters\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "num_epochs = 3\n",
        "\n",
        "# Prepare model, optimizer, and dataloader with the accelerator\n",
        "model, optimizer, train_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    for batch in progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# -----------------\n",
        "# 5. Visualization and Assessment (Simplified)\n",
        "# -----------------\n",
        "\n",
        "print(\"Training finished. Saving model...\")\n",
        "\n",
        "# Save the LoRA adapters and the full model.\n",
        "# The adapters are tiny and easy to share.\n",
        "peft_model_path = \"./videomae_lora_adapters\"\n",
        "model.save_pretrained(peft_model_path)\n",
        "print(f\"LoRA adapters saved to: {peft_model_path}\")\n",
        "\n",
        "# To visualize the results, we would normally use a validation set\n",
        "# and a downstream task like video classification.\n",
        "# For this tutorial, we will simply demonstrate a qualitative assessment\n",
        "# of the model's ability to reconstruct a masked video.\n",
        "\n",
        "print(\"\\n--- Model Assessment (Qualitative) ---\")\n",
        "\n",
        "# The model's primary objective is to reconstruct masked patches.\n",
        "# A lower loss during training indicates the model is getting better at this.\n",
        "# You can assess the reconstruction quality visually by grabbing the output\n",
        "# and visualizing the reconstructed video frames.\n",
        "\n",
        "# Let's take one example from the dataset and run it through the model.\n",
        "model.eval()\n",
        "sample_batch = next(iter(train_dataloader))\n",
        "sample_batch = {k: v.to(device) for k, v in sample_batch.items()}\n",
        "\n",
        "# Forward pass to get the reconstructed video patches\n",
        "with torch.no_grad():\n",
        "    outputs = model(**sample_batch)\n",
        "\n",
        "# The reconstructed pixel values are in `outputs.logits`.\n",
        "# The shapes can be complex, so we'll just check the output shape.\n",
        "reconstructed_pixels = outputs.logits\n",
        "print(f\"Shape of reconstructed pixels: {reconstructed_pixels.shape}\")\n",
        "print(\"The reconstructed pixels represent the model's guess for the masked video content.\")\n",
        "print(\"A well-trained model would produce a plausible reconstruction.\")\n",
        "\n",
        "# You could also add code here to visualize the original vs. reconstructed video frames\n",
        "# using a library like matplotlib or a custom function.\n",
        "# For example:\n",
        "#\n",
        "# import matplotlib.pyplot as plt\n",
        "#\n",
        "# original_frames = sample_batch['pixel_values'][0]\n",
        "# original_frames = processor.post_process_video_output(original_frames, output_norm=True)\n",
        "#\n",
        "# reconstructed_frames = reconstructed_pixels[0]\n",
        "# reconstructed_frames = processor.post_process_video_output(reconstructed_frames, output_norm=True)\n",
        "#\n",
        "# fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "# axes[0].imshow(original_frames[0].permute(1, 2, 0))\n",
        "# axes[0].set_title(\"Original Frame\")\n",
        "# axes[1].imshow(reconstructed_frames[0].permute(1, 2, 0))\n",
        "# axes[1].set_title(\"Reconstructed Frame\")\n",
        "# plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "9qgcy9KGVB4k"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}